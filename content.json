{"meta":{"title":"Zhiyu's blog","subtitle":"Make everything indexed.","description":null,"author":"Zhiyu Chen","url":"http://yoursite.com"},"pages":[{"title":"about me","date":"2016-01-28T23:29:49.000Z","updated":"2016-08-25T03:16:59.000Z","comments":true,"path":"about-me/index.html","permalink":"http://yoursite.com/about-me/index.html","excerpt":"","text":"My name is Zhiyu Chen, from China. I am a Ph.D student at Lehigh University CSE department, advised by Prof. Brian D. Davison and Prof. Mooi choo chuah. My research interests include data mining, machine learning， natural language processing and social network analysis."},{"title":"","date":"2018-11-21T03:33:56.950Z","updated":"2016-01-28T15:39:11.000Z","comments":true,"path":"instagram/index.html","permalink":"http://yoursite.com/instagram/index.html","excerpt":"","text":"layout: postslug: “instagram”title: “album” noDate: “true” from instagram，loading…"}],"posts":[{"title":"Reservoir sampling","slug":"Reservoir-sampling","date":"2017-09-01T17:10:08.000Z","updated":"2017-09-02T19:13:23.000Z","comments":true,"path":"2017/09/01/Reservoir-sampling/","link":"","permalink":"http://yoursite.com/2017/09/01/Reservoir-sampling/","excerpt":"","text":"Problem StatementRandomly choosing k items from a list S containing n items where n is too large or unknown. Method: Reservoir SamplingSteps: keep the first item in memory for the next item i (i &gt; 1) keep the new item with probability 1/i ignore the new item with probability 1 - 1/i It is easy to prove that when there are n items, each item is kept with probability 1/n. Python ImplementationSuppose we have a long document but only keep 10 tokens from the document. import random SAMPLE_COUNT = 10 random.seed(2333) doc = &apos;&apos;&apos;Reservoir sampling is a family of randomized algorithms for randomly choosing a sample of k items from a list S containing n items, where n is either a very large or unknown number. Typically n is large enough that the list doesn&apos;t fit into main memory.&apos;&apos;&apos; tokens = doc.split(&apos; &apos;) sample_tokens = [] for idx, token in enumerate(tokens): # Generate the reservoir if idx &lt; SAMPLE_COUNT: sample_tokens.append(token) else: # Randomly replace elements in the reservoir r = random.randint(0, idx) if r &lt; SAMPLE_COUNT: sample_tokens[r] = token print(sample_tokens)","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"http://yoursite.com/tags/algorithm/"}]},{"title":"dbpedia","slug":"dbpedia","date":"2017-04-27T15:12:37.000Z","updated":"2018-11-21T03:17:34.000Z","comments":true,"path":"2017/04/27/dbpedia/","link":"","permalink":"http://yoursite.com/2017/04/27/dbpedia/","excerpt":"","text":"Limits of DBPedia servicefrom usage report Connection limit of 50 parallel connections per IP address. Rate limit of 100 requests per second per IP address, with an initial burst of 120 requests. Ideally, applications should be written to check the HTTP status code of each request, and in case of a 503 status code, perform a 1–2 second sleep before retrying the request. Online Accessref We could use sparqlwrapper to make requests: from SPARQLWrapper import SPARQLWrapper, JSON sparql = SPARQLWrapper(&quot;http://dbpedia.org/sparql&quot;) sparql.setQuery(&quot;&quot;&quot; PREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt; PREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; PREFIX foaf: &lt;http://xmlns.com/foaf/0.1/&gt; PREFIX dc: &lt;http://purl.org/dc/elements/1.1/&gt; PREFIX : &lt;http://dbpedia.org/resource/&gt; PREFIX dbpedia2: &lt;http://dbpedia.org/property/&gt; PREFIX dbpedia: &lt;http://dbpedia.org/&gt; PREFIX skos: &lt;http://www.w3.org/2004/02/skos/core#&gt; SELECT ?abstract WHERE { &lt;http://dbpedia.org/resource/Asturias&gt; dbo:abstract ?abstract } &quot;&quot;&quot;) sparql.setReturnFormat(JSON) results = sparql.query().convert() print results[&apos;results&apos;][&apos;bindings&apos;][3][&apos;abstract&apos;][&apos;value&apos;] SPARQL Tutorial SPARQL By Example SPARQL By Example 2 DBpedia Tutorial","categories":[],"tags":[{"name":"Knowledge Base","slug":"Knowledge-Base","permalink":"http://yoursite.com/tags/Knowledge-Base/"},{"name":"DBPedia","slug":"DBPedia","permalink":"http://yoursite.com/tags/DBPedia/"}]},{"title":"Free tools for collecting data from social network","slug":"Free tools for collecting data from social network","date":"2016-08-24T21:27:04.000Z","updated":"2016-08-25T03:07:28.000Z","comments":true,"path":"2016/08/24/Free tools for collecting data from social network/","link":"","permalink":"http://yoursite.com/2016/08/24/Free tools for collecting data from social network/","excerpt":"","text":"BackTweetsBackTweets is a twitter time machine which enables you to search through a tweet history for tweets that link back to your site. Website: http://backtweets.com/ IcerocketEnter keywords to see mentions trended over time. Website: http://www.icerocket.com/ Webhose Website: https://webhose.io Python library: https://github.com/Buzzilla/webhose-python Social SearcherExport to csv files easily. Website: https://www.social-searcher.com/ TipTop Website: http://feeltiptop.com/ search results are grouped into images, videos, and sentiment polarities. Twazzupreal-time content from Twitter Website: http://twazzup.com/ TweetReachExport to csv files easily. Website: https://tweetreach.com Twitter Searchallow advanced queries Website: https://twitter.com/search-home","categories":[],"tags":[{"name":"Tools","slug":"Tools","permalink":"http://yoursite.com/tags/Tools/"},{"name":"Dataset","slug":"Dataset","permalink":"http://yoursite.com/tags/Dataset/"},{"name":"Data","slug":"Data","permalink":"http://yoursite.com/tags/Data/"},{"name":"Social Network Analysis","slug":"Social-Network-Analysis","permalink":"http://yoursite.com/tags/Social-Network-Analysis/"}]},{"title":"Common Knowledge Base","slug":"common knowledge bases","date":"2016-04-29T18:01:44.000Z","updated":"2016-05-17T14:42:57.000Z","comments":true,"path":"2016/04/29/common knowledge bases/","link":"","permalink":"http://yoursite.com/2016/04/29/common knowledge bases/","excerpt":"","text":"Recently, I read a lot of papers making use of Wikipedia and also other different knowledge bases. Therefore, I collect some common ones which may be helpful for future use. ConceptNetConceptNet is a semantic network containing lots of things computers should know about the world, especially when understanding text written by people. WordNeta lexical knowledge base of about 25,000 words grouped into an ontology of synsets Freebasea social database of 1,450 concepts WikiTaxonomya taxonomy of about 127,000 concepts extracted from Wikipedia’s category links ProBaseIt contains about 12 million concepts learned iteratively from 1.68 billion web pages in Bing web repository. GeoNamesIt contains over 10 million geographical names and consists of over 9 million unique features whereof 2.8 million populated places and 5.5 million alternate names. YAGOa semantic knowledge base of 149,162 instances derived from Wikipedia, WordNet and GeoNames, and ProBase","categories":[],"tags":[{"name":"Dataset","slug":"Dataset","permalink":"http://yoursite.com/tags/Dataset/"}]},{"title":"Learn bout Wikipedia","slug":"learn about wikipedia","date":"2016-03-11T22:02:11.000Z","updated":"2016-03-11T23:17:43.000Z","comments":true,"path":"2016/03/11/learn about wikipedia/","link":"","permalink":"http://yoursite.com/2016/03/11/learn about wikipedia/","excerpt":"","text":"1. MediaWiki Markup Languagelink 2. Tools for parsing Wikipedia Wikipedia Extractor Cloud9 other helps database download list of data processing tools about the dump","categories":[],"tags":[{"name":"Text Mining","slug":"Text-Mining","permalink":"http://yoursite.com/tags/Text-Mining/"},{"name":"Wikipedia","slug":"Wikipedia","permalink":"http://yoursite.com/tags/Wikipedia/"}]},{"title":"How to Build a Real-Time News Search Engine","slug":"Building-a-Real-Time-News-Search-Engine","date":"2016-03-10T05:00:53.000Z","updated":"2016-03-18T21:31:15.000Z","comments":true,"path":"2016/03/10/Building-a-Real-Time-News-Search-Engine/","link":"","permalink":"http://yoursite.com/2016/03/10/Building-a-Real-Time-News-Search-Engine/","excerpt":"","text":"Interesting Info from Bloombergblogyoutube Target delivers real-time search queries process alert requests Solution Using Luwak library Reduce the number of queries by “pre-searcher” Index documents into in-memory index (Lucene memory index). There could be extentions for scoring particular alerts. Real-time full-text search with Luwak and Samzalink Motivation Documents are changing Companies may want to monitor specific info Open Source Tools Luwak: a Lucene-based library for running many thousands of queries over a single document Samza: a stream processing framework based on Kafka Use Luwak and Samza to Build a Streaming Search EngineLuwak: do efficient streaming search first register queries with Luwak, and then match documents against them. Luwak tells you which of the registered queries match the document. index the queries The index is a dictionary from terms to queries: it maps terms to queries containing that term. processing the stream of documents For each document, we want to find all the queries that match. The trick is to take each document, and turn it into a query. Intuitively,“find me all the queries that match any of the words in this document”.","categories":[],"tags":[{"name":"Search Engine","slug":"Search-Engine","permalink":"http://yoursite.com/tags/Search-Engine/"},{"name":"Text Mining","slug":"Text-Mining","permalink":"http://yoursite.com/tags/Text-Mining/"}]},{"title":"A Net War Between Netizens from Mainland China and Taiwan On Facebook and Twitter","slug":"taiwan-netwar","date":"2016-02-17T22:28:32.000Z","updated":"2016-02-17T22:38:22.000Z","comments":true,"path":"2016/02/17/taiwan-netwar/","link":"","permalink":"http://yoursite.com/2016/02/17/taiwan-netwar/","excerpt":"","text":"coming soon …… Background http://www.cnn.com/2016/01/22/asia/taiwan-china-facebook-military-drills/ http://www.globaltimes.cn/content/965038.shtml http://blogs.wsj.com/chinarealtime/2016/01/21/chinese-netizens-flood-tsai-ing-wens-facebook-page-with-anti-taiwan-independence-posts/ http://qz.com/598812/an-army-of-chinese-trolls-has-jumped-the-great-firewall-to-attack-taiwanese-independence-on-facebook/","categories":[],"tags":[{"name":"Social Network","slug":"Social-Network","permalink":"http://yoursite.com/tags/Social-Network/"},{"name":"Data Science","slug":"Data-Science","permalink":"http://yoursite.com/tags/Data-Science/"}]},{"title":"Frequently Used Scala functions and Classes","slug":"scala-function","date":"2016-02-09T15:24:35.000Z","updated":"2016-02-14T22:47:42.000Z","comments":true,"path":"2016/02/09/scala-function/","link":"","permalink":"http://yoursite.com/2016/02/09/scala-function/","excerpt":"","text":"span() v.s. partition()partition will put all “true” elements in one list, and the others in the second list. span will put all elements in one list until an element is “false” (in terms of the predicate). From that point forward, it will put the elements in the second list. scala&gt; List(1,2,3,4).partition(x =&gt; x % 2 == 0) res0: (List[Int], List[Int]) = (List(2, 4),List(1, 3)) scala&gt; Seq(1,2,3,4).span(x =&gt; x % 2 == 0) res1: (Seq[Int], Seq[Int]) = (List(),List(1, 2, 3, 4)) flatten &amp; flatMapflatten: scala&gt; List(List(1, 2), List(3, 4)).flatten res0: List[Int] = List(1, 2, 3, 4) flatMap (map 1st, then flatten): scala&gt; val nestedNumbers = List(List(1, 2), List(3, 4)) nestedNumbers: List[List[Int]] = List(List(1, 2), List(3, 4)) scala&gt; nestedNumbers.flatMap(x =&gt; x.map(_ * 2)) res0: List[Int] = List(2, 4, 6, 8) collectAsMap()return the key-value pairs in this RDD to the master as a Map. If a key has multiple values, the preceding values will be covered. scala&gt; val data = sc.parallelize(List((1, &quot;www&quot;), (1, &quot;iteblog&quot;), (1, &quot;com&quot;), (2, &quot;bbs&quot;), (2, &quot;iteblog&quot;), (2, &quot;com&quot;), (3, &quot;good&quot;))) data: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:12 scala&gt; data.collectAsMap res28: scala.collection.Map[Int,String] = Map(2 -&gt; com, 1 -&gt; com, 3 -&gt; good) Map.getOrElse(key, default)return the value corresponding to key, or return default. example from Quora: scala&gt; val map = Map(&quot;Hi&quot; -&gt; &quot;Dan&quot;, &quot;Hello&quot; -&gt; null) map: scala.collection.immutable.Map[java.lang.String,java.lang.String] = Map((Hi,Dan), (Hello,null)) scala&gt; map.getOrElse(&quot;Hi&quot;, &quot;No key with that name!&quot;) res1: java.lang.String = Dan scala&gt; val result = map.getOrElse(&quot;Gutentag&quot;, &quot;No key with that name!&quot;) res2: java.lang.String = No key with that name! mkStringConverting a Collection to a String scala&gt; val a = Array(&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;) scala&gt; a.mkString res1: String = applebananacherry scala&gt; a.mkString(&quot;, &quot;) res3: String = apple, banana, cherry ref mapValues &amp; transformLet’s say we have a immutable Map[A,B]. mapValues takes a function B =&gt; C, where C is the new type for the values. transform takes a function (A, B) =&gt; C, where this C is also the type for the values. However with the transform function you can influence the result of the new values by the value of their keys. val m = Map( &quot;a&quot; -&gt; 2, &quot;b&quot; -&gt; 3 ) m.transform((key, value) =&gt; key + value) //Map[String, String](a -&gt; a2, b -&gt; b3) refref2 ##groupby http://markusjais.com/the-groupby-method-from-scalas-collection-library/ ziphttp://www.iteblog.com/archives/1225 Class StatCounterhttps://spark.apache.org/docs/1.2.1/api/java/org/apache/spark/util/StatCounter.html","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/tags/Scala/"}]},{"title":"Install Scala","slug":"install-scala","date":"2016-02-02T01:44:28.000Z","updated":"2016-02-14T22:47:38.000Z","comments":true,"path":"2016/02/01/install-scala/","link":"","permalink":"http://yoursite.com/2016/02/01/install-scala/","excerpt":"","text":"Take Scala 2.11.7 for example. Download Scala 2.11.7 from http://www.scala-lang.org/download/ decompress and move $ tar zxf scala-2.11.7.tgz $ sudo mv scala-2.11.7 /usr/share/scala link scala libraries $ sudo ln -s /usr/share/scala/bin/scala /usr/bin/scala $ sudo ln -s /usr/share/scala/bin/scalac /usr/bin/scalac $ sudo ln -s /usr/share/scala/bin/fsc /usr/bin/fsc $ sudo ln -s /usr/share/scala/bin/sbaz /usr/bin/sbaz $ sudo ln -s /usr/share/scala/bin/sbaz-setup /usr/bin/sbaz-setup $ sudo ln -s /usr/share/scala/bin/scaladoc /usr/bin/scaladoc $ sudo ln -s /usr/share/scala/bin/scalap /usr/bin/scalap Uninstall: $ sudo rm -rf /usr/share/scala /usr/bin/scala /usr/bin/scalac /usr/bin/fsc /usr/bin/sbaz /usr/bin/sbaz-setup /usr/bin/scaladoc /usr/bin/scalap","categories":[],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://yoursite.com/tags/Scala/"}]},{"title":"Hello World","slug":"hello-world","date":"2016-01-28T23:29:49.000Z","updated":"2016-02-14T22:47:33.000Z","comments":true,"path":"2016/01/28/hello-world/","link":"","permalink":"http://yoursite.com/2016/01/28/hello-world/","excerpt":"","text":"This is my new blog.Previously, I usually took notes on gitbook because of its simplicity and allowing for systematic organization of content. Considering the chance of sharing something, I am trying to move here.","categories":[],"tags":[{"name":"Lifestyle","slug":"Lifestyle","permalink":"http://yoursite.com/tags/Lifestyle/"}]}]}